{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alinemsm/CS221-project/blob/master/colab_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDEExiAk4fLb"
      },
      "source": [
        "# Fine-tune Gemma 2b using LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1q6-W_mKIT-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0_EdOg9DPK6Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata, drive"
      ]
    },
    {
      "metadata": {
        "id": "eAOqB4S6vv8b"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 2,
      "source": [
        "COLAB = True\n",
        "KAGGLE = True\n",
        "DOWNLOAD_DATA = True\n",
        "SAVE_TO_GITHUB = True\n",
        "GIT_REPOSITORY = \"CS221-project\"\n",
        "FILE_NAME = \"colab_tuning.ipynb\"\n"
      ]
    },
    {
      "metadata": {
        "id": "fKsL8ZfCvv8b",
        "outputId": "7dc87048-a56c-4fbf-d6a8-57711746048f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 3,
      "source": [
        "if COLAB:\n",
        "    %cd /content\n",
        "    drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "metadata": {
        "id": "IKHYlV8ovv8b",
        "outputId": "cbc60a43-7549-4bd8-e933-033629ba9cd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "execution_count": 4,
      "source": [
        "if COLAB:\n",
        "    PARENT_DIRECTORY_PATH = \"/content\"\n",
        "    # In case you want to clone in your drive:\n",
        "    PARENT_DIRECTORY_PATH = \"/content/drive/MyDrive\"\n",
        "    PROJECT_PATH = PARENT_DIRECTORY_PATH + \"/\" + GIT_REPOSITORY\n",
        "    %cd \"{PARENT_DIRECTORY_PATH}\""
      ]
    },
    {
      "metadata": {
        "id": "ycjSLQITvv8c",
        "outputId": "d0839a65-2b6c-4a31-af6c-34d2ace9956e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/CS221-project\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 8 (delta 4), reused 5 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), 692.85 KiB | 299.00 KiB/s, done.\n",
            "From https://github.com/alinemsm/CS221-project\n",
            "   c38de7d..d28e245  master     -> origin/master\n",
            "Updating c38de7d..d28e245\n",
            "error: Your local changes to the following files would be overwritten by merge:\n",
            "\tcolab_tuning.ipynb\n",
            "Please commit your changes or stash them before you merge.\n",
            "Aborting\n"
          ]
        }
      ],
      "execution_count": 5,
      "source": [
        "if COLAB:\n",
        "    import json\n",
        "    import os\n",
        "\n",
        "    with open(f\"{PARENT_DIRECTORY_PATH}/Git/git.json\", \"r\") as f:\n",
        "        parsed_json = json.load(f)\n",
        "\n",
        "    GIT_USER_NAME = parsed_json[\"GIT_USER_NAME\"]\n",
        "    GIT_TOKEN = parsed_json[\"GIT_TOKEN\"]\n",
        "    GIT_USER_EMAIL = parsed_json[\"GIT_USER_EMAIL\"]\n",
        "\n",
        "    GIT_PATH = (\n",
        "        f\"https://{GIT_TOKEN}@github.com/{GIT_USER_NAME}/{GIT_REPOSITORY}.git\"\n",
        "    )\n",
        "\n",
        "    %cd \"{PARENT_DIRECTORY_PATH}\"\n",
        "\n",
        "    if os.path.exists(f\"{PARENT_DIRECTORY_PATH}/{GIT_REPOSITORY}\"):\n",
        "        %cd \"{PROJECT_PATH}\"\n",
        "        !git pull\n",
        "    else:\n",
        "        !git clone \"{GIT_PATH}\"  # Clone the github repository\n",
        "        %cd \"{PROJECT_PATH}\""
      ]
    },
    {
      "metadata": {
        "id": "ntzzF8ouvv8c"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "if COLAB:\n",
        "    import os\n",
        "    os.environ[\"KAGGLE_CONFIG_DIR\"] = f\"{PARENT_DIRECTORY_PATH}/Kaggle/kaggle.json\""
      ]
    },
    {
      "metadata": {
        "id": "8qNLYh3Ivv8d"
      },
      "cell_type": "markdown",
      "source": [
        "### Set environment variables"
      ]
    },
    {
      "metadata": {
        "id": "WnmItm6gvv8d"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "\n",
        "# Read the kaggle.json file\n",
        "# with open(\"kaggle.json\") as f:\n",
        "#     kaggle_info = json.load(f)\n",
        "\n",
        "# Set the environment variables\n",
        "# os.environ[\"KAGGLE_USERNAME\"] = kaggle_info[\"username\"]\n",
        "# os.environ[\"KAGGLE_KEY\"] = kaggle_info[\"key\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuEUAKJW1QkQ"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eeBtYqJsZPG"
      },
      "outputs": [],
      "source": [
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U keras>=3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGLS-l5TxIR4"
      },
      "source": [
        "### Select a backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn5uy8X8sdD0"
      },
      "outputs": [],
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
        "# Avoid memory fragmentation on JAX backend.\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZs8XXqUKRmi"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYHyPUA9hKTf"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T7xe_jzslv4"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45UpBDfBgf0I"
      },
      "source": [
        "Preprocess the data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "LHBvCsU3zPWK",
        "outputId": "255f76fa-a1dc-4b29-db38-a59af401d400",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic"
      ],
      "metadata": {
        "id": "8AELgZC9zD09",
        "outputId": "96a38ed1-c2b2-4951-b2a0-a3f8a3d3a1b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.10/dist-packages (0.26.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.4.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.18.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic) (0.23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from JSON file\n",
        "import json\n",
        "with open(\"/content/drive/MyDrive/CS221-project/qa_test_data.json\", \"r\") as file:\n",
        "    contents = json.load(file)"
      ],
      "metadata": {
        "id": "1F_WwWHaTBSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_format(input_data):\n",
        "    output_data = []\n",
        "    for item in input_data:\n",
        "        formatted_string = f\"Instruction:\\n{item['instruction']}\\nResponse:\\n{item['response']}\"\n",
        "        output_data.append(formatted_string)\n",
        "    return output_data\n",
        "\n",
        "# Convert the format\n",
        "data = convert_format(contents)\n"
      ],
      "metadata": {
        "id": "8POMxFcUTMzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RCE3fdGhDE5"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz5zLEyLstfn",
        "outputId": "66ef802e-be3e-43a6-8a51-f94e672cc769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'task.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'preprocessor.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_L6A5J-1QgC"
      },
      "source": [
        "## Inference before fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "def compute_rouge_l(reference, hypothesis):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(hypothesis, reference)\n",
        "    return scores[0]['rouge-l']['f']"
      ],
      "metadata": {
        "id": "dRGVDKTd9KkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from utils import compute_rouge_l\n",
        "\n",
        "# Load data from JSON file\n",
        "with open(\"/content/drive/MyDrive/CS221-project/qa_test_data.json\", \"r\") as file:\n",
        "    test_data = json.load(file)"
      ],
      "metadata": {
        "id": "Lgs4JABU9L7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the template\n",
        "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "\n",
        "# Define the sampler\n",
        "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "\n",
        "results = []\n",
        "rouge_l_scores = []\n",
        "i = 0\n",
        "# Generate responses and compute ROUGE-L metric\n",
        "for item in test_data:\n",
        "    prompt = template.format(instruction=item[\"instruction\"], response=\"\")\n",
        "    model_response = gemma_lm.generate(prompt, max_length=256)\n",
        "\n",
        "    rouge_l_score = compute_rouge_l(item[\"response\"], model_response)\n",
        "    rouge_l_scores.append(rouge_l_score)\n",
        "\n",
        "    result = {\n",
        "        \"instruction\": item[\"instruction\"],\n",
        "        \"model_response\": model_response,\n",
        "        \"original_response\": item[\"response\"],\n",
        "        \"rouge_l_score\": rouge_l_score\n",
        "    }\n",
        "    i += 1\n",
        "    print(f\"Completed {i}.\")\n",
        "    results.append(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq9zyKst8Wga",
        "outputId": "cfb6ca67-d8a6-40c3-8988-ac0d8fbd4218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 1.\n",
            "Completed 2.\n",
            "Completed 3.\n",
            "Completed 4.\n",
            "Completed 5.\n",
            "Completed 6.\n",
            "Completed 7.\n",
            "Completed 8.\n",
            "Completed 9.\n",
            "Completed 10.\n",
            "Completed 11.\n",
            "Completed 12.\n",
            "Completed 13.\n",
            "Completed 14.\n",
            "Completed 15.\n",
            "Completed 16.\n",
            "Completed 17.\n",
            "Completed 18.\n",
            "Completed 19.\n",
            "Completed 20.\n",
            "Completed 21.\n",
            "Completed 22.\n",
            "Completed 23.\n",
            "Completed 24.\n",
            "Completed 25.\n",
            "Completed 26.\n",
            "Completed 27.\n",
            "Completed 28.\n",
            "Completed 29.\n",
            "Completed 30.\n",
            "Completed 31.\n",
            "Completed 32.\n",
            "Completed 33.\n",
            "Completed 34.\n",
            "Completed 35.\n",
            "Completed 36.\n",
            "Completed 37.\n",
            "Completed 38.\n",
            "Completed 39.\n",
            "Completed 40.\n",
            "Completed 41.\n",
            "Completed 42.\n",
            "Completed 43.\n",
            "Completed 44.\n",
            "Completed 45.\n",
            "Completed 46.\n",
            "Completed 47.\n",
            "Completed 48.\n",
            "Completed 49.\n",
            "Completed 50.\n",
            "Completed 51.\n",
            "Completed 52.\n",
            "Completed 53.\n",
            "Completed 54.\n",
            "Completed 55.\n",
            "Completed 56.\n",
            "Completed 57.\n",
            "Completed 58.\n",
            "Completed 59.\n",
            "Completed 60.\n",
            "Completed 61.\n",
            "Completed 62.\n",
            "Completed 63.\n",
            "Completed 64.\n",
            "Completed 65.\n",
            "Completed 66.\n",
            "Completed 67.\n",
            "Completed 68.\n",
            "Completed 69.\n",
            "Completed 70.\n",
            "Completed 71.\n",
            "Completed 72.\n",
            "Completed 73.\n",
            "Completed 74.\n",
            "Completed 75.\n",
            "Completed 76.\n",
            "Completed 77.\n",
            "Completed 78.\n",
            "Completed 79.\n",
            "Completed 80.\n",
            "Completed 81.\n",
            "Completed 82.\n",
            "Completed 83.\n",
            "Completed 84.\n",
            "Completed 85.\n",
            "Completed 86.\n",
            "Completed 87.\n",
            "Completed 88.\n",
            "Completed 89.\n",
            "Completed 90.\n",
            "Completed 91.\n",
            "Completed 92.\n",
            "Completed 93.\n",
            "Completed 94.\n",
            "Completed 95.\n",
            "Completed 96.\n",
            "Completed 97.\n",
            "Completed 98.\n",
            "Completed 99.\n",
            "Completed 100.\n",
            "Completed 101.\n",
            "Completed 102.\n",
            "Completed 103.\n",
            "Completed 104.\n",
            "Completed 105.\n",
            "Completed 106.\n",
            "Completed 107.\n",
            "Completed 108.\n",
            "Completed 109.\n",
            "Completed 110.\n",
            "Completed 111.\n",
            "Completed 112.\n",
            "Completed 113.\n",
            "Completed 114.\n",
            "Completed 115.\n",
            "Completed 116.\n",
            "Completed 117.\n",
            "Completed 118.\n",
            "Completed 119.\n",
            "Completed 120.\n",
            "Completed 121.\n",
            "Completed 122.\n",
            "Completed 123.\n",
            "Completed 124.\n",
            "Completed 125.\n",
            "Completed 126.\n",
            "Completed 127.\n",
            "Completed 128.\n",
            "Completed 129.\n",
            "Completed 130.\n",
            "Completed 131.\n",
            "Completed 132.\n",
            "Completed 133.\n",
            "Completed 134.\n",
            "Completed 135.\n",
            "Completed 136.\n",
            "Completed 137.\n",
            "Completed 138.\n",
            "Completed 139.\n",
            "Completed 140.\n",
            "Completed 141.\n",
            "Completed 142.\n",
            "Completed 143.\n",
            "Completed 144.\n",
            "Completed 145.\n",
            "Completed 146.\n",
            "Completed 147.\n",
            "Completed 148.\n",
            "Completed 149.\n",
            "Completed 150.\n",
            "Completed 151.\n",
            "Completed 152.\n",
            "Completed 153.\n",
            "Completed 154.\n",
            "Completed 155.\n",
            "Completed 156.\n",
            "Completed 157.\n",
            "Completed 158.\n",
            "Completed 159.\n",
            "Completed 160.\n",
            "Completed 161.\n",
            "Completed 162.\n",
            "Completed 163.\n",
            "Completed 164.\n",
            "Completed 165.\n",
            "Completed 166.\n",
            "Completed 167.\n",
            "Completed 168.\n",
            "Completed 169.\n",
            "Completed 170.\n",
            "Completed 171.\n",
            "Completed 172.\n",
            "Completed 173.\n",
            "Completed 174.\n",
            "Completed 175.\n",
            "Completed 176.\n",
            "Completed 177.\n",
            "Completed 178.\n",
            "Completed 179.\n",
            "Completed 180.\n",
            "Completed 181.\n",
            "Completed 182.\n",
            "Completed 183.\n",
            "Completed 184.\n",
            "Completed 185.\n",
            "Completed 186.\n",
            "Completed 187.\n",
            "Completed 188.\n",
            "Completed 189.\n",
            "Completed 190.\n",
            "Completed 191.\n",
            "Completed 192.\n",
            "Completed 193.\n",
            "Completed 194.\n",
            "Completed 195.\n",
            "Completed 196.\n",
            "Completed 197.\n",
            "Completed 198.\n",
            "Completed 199.\n",
            "Completed 200.\n",
            "Completed 201.\n",
            "Completed 202.\n",
            "Completed 203.\n",
            "Completed 204.\n",
            "Completed 205.\n",
            "Completed 206.\n",
            "Completed 207.\n",
            "Completed 208.\n",
            "Completed 209.\n",
            "Completed 210.\n",
            "Completed 211.\n",
            "Completed 212.\n",
            "Completed 213.\n",
            "Completed 214.\n",
            "Completed 215.\n",
            "Completed 216.\n",
            "Completed 217.\n",
            "Completed 218.\n",
            "Completed 219.\n",
            "Completed 220.\n",
            "Completed 221.\n",
            "Completed 222.\n",
            "Completed 223.\n",
            "Completed 224.\n",
            "Completed 225.\n",
            "Completed 226.\n",
            "Completed 227.\n",
            "Completed 228.\n",
            "Completed 229.\n",
            "Completed 230.\n",
            "Completed 231.\n",
            "Completed 232.\n",
            "Completed 233.\n",
            "Completed 234.\n",
            "Completed 235.\n",
            "Completed 236.\n",
            "Completed 237.\n",
            "Completed 238.\n",
            "Completed 239.\n",
            "Completed 240.\n",
            "Completed 241.\n",
            "Completed 242.\n",
            "Completed 243.\n",
            "Completed 244.\n",
            "Completed 245.\n",
            "Completed 246.\n",
            "Completed 247.\n",
            "Completed 248.\n",
            "Completed 249.\n",
            "Completed 250.\n",
            "Completed 251.\n",
            "Completed 252.\n",
            "Completed 253.\n",
            "Completed 254.\n",
            "Completed 255.\n",
            "Completed 256.\n",
            "Completed 257.\n",
            "Completed 258.\n",
            "Completed 259.\n",
            "Completed 260.\n",
            "Completed 261.\n",
            "Completed 262.\n",
            "Completed 263.\n",
            "Completed 264.\n",
            "Completed 265.\n",
            "Completed 266.\n",
            "Completed 267.\n",
            "Completed 268.\n",
            "Completed 269.\n",
            "Completed 270.\n",
            "Completed 271.\n",
            "Completed 272.\n",
            "Completed 273.\n",
            "Completed 274.\n",
            "Completed 275.\n",
            "Completed 276.\n",
            "Completed 277.\n",
            "Completed 278.\n",
            "Completed 279.\n",
            "Completed 280.\n",
            "Completed 281.\n",
            "Completed 282.\n",
            "Completed 283.\n",
            "Completed 284.\n",
            "Completed 285.\n",
            "Completed 286.\n",
            "Completed 287.\n",
            "Completed 288.\n",
            "Completed 289.\n",
            "Completed 290.\n",
            "Completed 291.\n",
            "Completed 292.\n",
            "Completed 293.\n",
            "Completed 294.\n",
            "Completed 295.\n",
            "Completed 296.\n",
            "Completed 297.\n",
            "Completed 298.\n",
            "Completed 299.\n",
            "Completed 300.\n",
            "Completed 301.\n",
            "Completed 302.\n",
            "Completed 303.\n",
            "Completed 304.\n",
            "Completed 305.\n",
            "Completed 306.\n",
            "Completed 307.\n",
            "Completed 308.\n",
            "Completed 309.\n",
            "Completed 310.\n",
            "Completed 311.\n",
            "Completed 312.\n",
            "Completed 313.\n",
            "Completed 314.\n",
            "Completed 315.\n",
            "Completed 316.\n",
            "Completed 317.\n",
            "Completed 318.\n",
            "Completed 319.\n",
            "Completed 320.\n",
            "Completed 321.\n",
            "Completed 322.\n",
            "Completed 323.\n",
            "Completed 324.\n",
            "Completed 325.\n",
            "Completed 326.\n",
            "Completed 327.\n",
            "Completed 328.\n",
            "Completed 329.\n",
            "Completed 330.\n",
            "Completed 331.\n",
            "Completed 332.\n",
            "Completed 333.\n",
            "Completed 334.\n",
            "Completed 335.\n",
            "Completed 336.\n",
            "Completed 337.\n",
            "Completed 338.\n",
            "Completed 339.\n",
            "Completed 340.\n",
            "Completed 341.\n",
            "Completed 342.\n",
            "Completed 343.\n",
            "Completed 344.\n",
            "Completed 345.\n",
            "Completed 346.\n",
            "Completed 347.\n",
            "Completed 348.\n",
            "Completed 349.\n",
            "Completed 350.\n",
            "Completed 351.\n",
            "Completed 352.\n",
            "Completed 353.\n",
            "Completed 354.\n",
            "Completed 355.\n",
            "Completed 356.\n",
            "Completed 357.\n",
            "Completed 358.\n",
            "Completed 359.\n",
            "Completed 360.\n",
            "Completed 361.\n",
            "Completed 362.\n",
            "Completed 363.\n",
            "Completed 364.\n",
            "Completed 365.\n",
            "Completed 366.\n",
            "Completed 367.\n",
            "Completed 368.\n",
            "Completed 369.\n",
            "Completed 370.\n",
            "Completed 371.\n",
            "Completed 372.\n",
            "Completed 373.\n",
            "Completed 374.\n",
            "Completed 375.\n",
            "Completed 376.\n",
            "Completed 377.\n",
            "Completed 378.\n",
            "Completed 379.\n",
            "Completed 380.\n",
            "Completed 381.\n",
            "Completed 382.\n",
            "Completed 383.\n",
            "Completed 384.\n",
            "Completed 385.\n",
            "Completed 386.\n",
            "Completed 387.\n",
            "Completed 388.\n",
            "Completed 389.\n",
            "Completed 390.\n",
            "Completed 391.\n",
            "Completed 392.\n",
            "Completed 393.\n",
            "Completed 394.\n",
            "Completed 395.\n",
            "Completed 396.\n",
            "Completed 397.\n",
            "Completed 398.\n",
            "Completed 399.\n",
            "Completed 400.\n",
            "Completed 401.\n",
            "Completed 402.\n",
            "Completed 403.\n",
            "Completed 404.\n",
            "Completed 405.\n",
            "Completed 406.\n",
            "Completed 407.\n",
            "Completed 408.\n",
            "Completed 409.\n",
            "Completed 410.\n",
            "Completed 411.\n",
            "Completed 412.\n",
            "Completed 413.\n",
            "Completed 414.\n",
            "Completed 415.\n",
            "Completed 416.\n",
            "Completed 417.\n",
            "Completed 418.\n",
            "Completed 419.\n",
            "Completed 420.\n",
            "Completed 421.\n",
            "Completed 422.\n",
            "Completed 423.\n",
            "Completed 424.\n",
            "Completed 425.\n",
            "Completed 426.\n",
            "Completed 427.\n",
            "Completed 428.\n",
            "Completed 429.\n",
            "Completed 430.\n",
            "Completed 431.\n",
            "Completed 432.\n",
            "Completed 433.\n",
            "Completed 434.\n",
            "Completed 435.\n",
            "Completed 436.\n",
            "Completed 437.\n",
            "Completed 438.\n",
            "Completed 439.\n",
            "Completed 440.\n",
            "Completed 441.\n",
            "Completed 442.\n",
            "Completed 443.\n",
            "Completed 444.\n",
            "Completed 445.\n",
            "Completed 446.\n",
            "Completed 447.\n",
            "Completed 448.\n",
            "Completed 449.\n",
            "Completed 450.\n",
            "Completed 451.\n",
            "Completed 452.\n",
            "Completed 453.\n",
            "Completed 454.\n",
            "Completed 455.\n",
            "Completed 456.\n",
            "Completed 457.\n",
            "Completed 458.\n",
            "Completed 459.\n",
            "Completed 460.\n",
            "Completed 461.\n",
            "Completed 462.\n",
            "Completed 463.\n",
            "Completed 464.\n",
            "Completed 465.\n",
            "Completed 466.\n",
            "Completed 467.\n",
            "Completed 468.\n",
            "Completed 469.\n",
            "Completed 470.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to a file\n",
        "with open(\"baseline_evaluation.json\", \"w\") as outfile:\n",
        "    json.dump(results, outfile, indent=4)\n",
        "\n",
        "# Output average ROUGE-L metric\n",
        "average_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
        "print(f\"Average ROUGE-L Metric: {average_rouge_l}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL8eEcB38eJn",
        "outputId": "df76c4d6-7e1f-42c4-b6a3-0f0603296b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average ROUGE-L Metric: 0.2801402634351348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "# Extract the rouge_l_score values\n",
        "rouge_l_scores = [item[\"rouge_l_score\"] for item in results]\n",
        "\n",
        "# Compute basic statistics\n",
        "min_score = min(rouge_l_scores)\n",
        "max_score = max(rouge_l_scores)\n",
        "avg_score = statistics.mean(rouge_l_scores)\n",
        "median_score = statistics.median(rouge_l_scores)\n",
        "std_dev_score = statistics.stdev(rouge_l_scores)\n",
        "\n",
        "# Print the computed statistics\n",
        "print(f\"Minimum ROUGE-L Score: {min_score}\")\n",
        "print(f\"Maximum ROUGE-L Score: {max_score}\")\n",
        "print(f\"Average ROUGE-L Score: {avg_score}\")\n",
        "print(f\"Median ROUGE-L Score: {median_score}\")\n",
        "print(f\"Standard Deviation of ROUGE-L Score: {std_dev_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTUyRaa-QFqu",
        "outputId": "4417d21a-056d-4d82-cf94-b5ed06cc7015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum ROUGE-L Score: 0.11382113341793927\n",
            "Maximum ROUGE-L Score: 0.45801526249286173\n",
            "Average ROUGE-L Score: 0.2801402634351348\n",
            "Median ROUGE-L Score: 0.2790697625298961\n",
            "Standard Deviation of ROUGE-L Score: 0.06289678901581437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from JSON file\n",
        "with open(\"/content/drive/MyDrive/CS221-project/fine_tuned_evaluation.json\", \"r\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract the rouge_l_score values\n",
        "rouge_l_scores = [item[\"rouge_l_score\"] for item in data]\n",
        "\n",
        "# Compute basic statistics\n",
        "min_score = min(rouge_l_scores)\n",
        "max_score = max(rouge_l_scores)\n",
        "avg_score = statistics.mean(rouge_l_scores)\n",
        "median_score = statistics.median(rouge_l_scores)\n",
        "std_dev_score = statistics.stdev(rouge_l_scores)\n",
        "\n",
        "# Print the computed statistics\n",
        "print(f\"Minimum ROUGE-L Score: {min_score}\")\n",
        "print(f\"Maximum ROUGE-L Score: {max_score}\")\n",
        "print(f\"Average ROUGE-L Score: {avg_score}\")\n",
        "print(f\"Median ROUGE-L Score: {median_score}\")\n",
        "print(f\"Standard Deviation of ROUGE-L Score: {std_dev_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq1zaRA1QVyp",
        "outputId": "e278f40d-7c05-4045-c9fd-59efdef0a52d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum ROUGE-L Score: 0.0759493625893289\n",
            "Maximum ROUGE-L Score: 0.6388888839236112\n",
            "Average ROUGE-L Score: 0.3035444978283638\n",
            "Median ROUGE-L Score: 0.2978723354362206\n",
            "Standard Deviation of ROUGE-L Score: 0.07007060324815784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.2978723354362206/0.2790697625298961"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPna2JC0Oz3j",
        "outputId": "f45074b5-890e-4232-f4e0-fd605a85e3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0673758874335595"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.3035444978283638/0.2801402634351348"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWEyz6q9UqFN",
        "outputId": "9b5641a8-c85c-4df4-ad00-5886a22c9eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0835447004519867"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.07007060324815784/0.06289678901581437"
      ],
      "metadata": {
        "id": "35i9Kb58VgFY",
        "outputId": "3dcd94e1-5759-4dbc-f2d7-ea69bdabcbaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1140569231688398"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(rouge_l_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHcGn_vYOv__",
        "outputId": "85e4b824-b913-4e20-fb72-605171e29682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "470"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVLXadptyo34"
      },
      "source": [
        "### Probability Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwQz3xxxKciD",
        "outputId": "871d4e0a-e707-4362-cea9-aca3a6715384",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "What is the difference between permutations and combinations?\n",
            "\n",
            "Response:\n",
            "There are 16 ways to choose 3 people from a group of 6.\n",
            "There are 12 ways to choose 3 people without repetition from a group of 9.\n",
            "There are 48 ways to choose 3 people with repetition from a group of 6.\n",
            "There are 10 ways to choose 3 people from a group of 3 with repetition.\n",
            "The answer is the last one.\n",
            "The number of combinations is the same as the number of permutations with repetition.\n",
            "There are 48 combinations of 3 people in a group of 6 with repetition.\n",
            "\n",
            "Explanation:\n",
            "\n",
            "1. There are 16 ways to choose 3 people from a group of 6.\n",
            "\n",
            "2. There are 12 ways to choose 3 people without repetition from a group of 9.\n",
            "\n",
            "3. There are 48 ways to choose 3 people with repetition from a group of 6.\n",
            "There are 10 ways to choose 3 people from a group of 3 with repetition.\n",
            "\n",
            "4.\n",
            "\n",
            "The number of permutations is the same as the number of combinations.\n",
            "\n",
            "The number of permutations with\n"
          ]
        }
      ],
      "source": [
        "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "\n",
        "prompt = template.format(\n",
        "    instruction=\"What is the difference between permutations and combinations?\",\n",
        "    response=\"\",\n",
        ")\n",
        "\n",
        "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ74Zz_S0iVv"
      },
      "source": [
        "### Supervised Learning Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lorJMbsusgoo",
        "outputId": "7acc803e-3d88-4174-e6e4-48457b1684c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "What is Supervised Learning?\n",
            "\n",
            "Response:\n",
            "Supervised learning is a type of machine learning in which the algorithm learns to map from a data set to a class label. In other words, it takes an input and produces a predicted output. The algorithm is given a set of training data with known class labels (e.g. 0 and 1 for binary classification). The goal is for the algorithm to predict the class label of new input data with the same accuracy as the training data.\n",
            "\n",
            "Supervised learning is used in many fields, including computer vision, natural language processing, and medical imaging. The most common supervised learning algorithms are linear models, decision trees, and artificial neural networks.\n"
          ]
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"What is Supervised Learning?\",\n",
        "    response=\"\",\n",
        ")\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt7Nr6a7tItO"
      },
      "source": [
        "## LoRA Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCucu6oHz53G",
        "outputId": "7da089e9-03de-4cff-ffb7-5048ca050a2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Enable LoRA for the model and set the LoRA rank to 4.\n",
        "gemma_lm.backbone.enable_lora(rank=4)\n",
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Peq7TnLtHse",
        "outputId": "19eded62-00c5-471c-cfb5-da9c4ff764c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1877/1877\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2426s\u001b[0m 1s/step - loss: 0.4666 - sparse_categorical_accuracy: 0.6152\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79d9b06f5150>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Limit the input sequence length to 512 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = 512\n",
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "gemma_lm.fit(data, epochs=1, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "gemma_lm.save(\"/content/drive/MyDrive/Colab Notebooks/cs221/fine_tuned_model_1.keras\")\n"
      ],
      "metadata": {
        "id": "3f_W8l02mukQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0lHxEDX03gp"
      },
      "outputs": [],
      "source": [
        "# Uncomment the line below if you want to enable mixed precision training on GPUs\n",
        "# keras.mixed_precision.set_global_policy('mixed_bfloat16')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model\n",
        "\n",
        "# loaded_model = keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/cs221/fine_tuned_model_1.keras\")\n",
        "\n",
        "# Use the loaded model for generation\n",
        "# prompt = template.format(\n",
        "#     instruction=\"What is Supervised Learning?\",\n",
        "#     response=\"\",\n",
        "# )\n",
        "# sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "# loaded_model.compile(sampler=sampler)\n",
        "# generated_text = loaded_model.generate(prompt, max_length=256)\n",
        "# print(generated_text)"
      ],
      "metadata": {
        "id": "yTLaxefSnBFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUsyCzONfRuJ",
        "outputId": "96aa46a3-f455-4a26-ced9-1917e5decbc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "def compute_rouge_l(reference, hypothesis):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(hypothesis, reference)\n",
        "    return scores[0]['rouge-l']['f']"
      ],
      "metadata": {
        "id": "hT5eNpn8fM5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from utils import compute_rouge_l\n",
        "\n",
        "# Load data from JSON file\n",
        "with open(\"qa_test_data.json\", \"r\") as file:\n",
        "    test_data = json.load(file)\n"
      ],
      "metadata": {
        "id": "H1hBSh7Ye-pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for item in test_data[:1]:\n",
        "#     prompt = template.format(instruction=item[\"instruction\"], response=\"\")\n",
        "#     model_response = gemma_lm.generate(prompt, max_length=256)"
      ],
      "metadata": {
        "id": "lF2Dr0Estg_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "o1Zrrj0Mtnko",
        "outputId": "a005b6ae-6b9f-4309-f2fc-d70893d9cd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Instruction:\\nHow can the class probabilities be estimated using the fitted function from the binomial log-likelihood and kernel functions?\\n\\nResponse:\\nThe fitted function from the binomial log-likelihood and kernel functions can be used to estimate the class probabilities of a test case. This is done by computing the kernel functions at the locations of the test cases and multiplying them by the estimated class probabilities for each kernel function. The sum of these products over all test cases provides an estimate of the class probabilities for each class, which can then be used to predict the class of the test case.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the template\n",
        "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "\n",
        "# Define the sampler\n",
        "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "\n",
        "results = []\n",
        "rouge_l_scores = []\n",
        "i = 0\n",
        "# Generate responses and compute ROUGE-L metric\n",
        "for item in test_data:\n",
        "    prompt = template.format(instruction=item[\"instruction\"], response=\"\")\n",
        "    model_response = gemma_lm.generate(prompt, max_length=256)\n",
        "\n",
        "    rouge_l_score = compute_rouge_l(item[\"response\"], model_response)\n",
        "    rouge_l_scores.append(rouge_l_score)\n",
        "\n",
        "    result = {\n",
        "        \"instruction\": item[\"instruction\"],\n",
        "        \"model_response\": model_response,\n",
        "        \"original_response\": item[\"response\"],\n",
        "        \"rouge_l_score\": rouge_l_score\n",
        "    }\n",
        "    i += 1\n",
        "    print(f\"Completed {i}.\")\n",
        "    results.append(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rKWldHMeu4l",
        "outputId": "14d08c68-cf53-4ed6-f29f-7b38e0a9bdc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 1.\n",
            "Completed 2.\n",
            "Completed 3.\n",
            "Completed 4.\n",
            "Completed 5.\n",
            "Completed 6.\n",
            "Completed 7.\n",
            "Completed 8.\n",
            "Completed 9.\n",
            "Completed 10.\n",
            "Completed 11.\n",
            "Completed 12.\n",
            "Completed 13.\n",
            "Completed 14.\n",
            "Completed 15.\n",
            "Completed 16.\n",
            "Completed 17.\n",
            "Completed 18.\n",
            "Completed 19.\n",
            "Completed 20.\n",
            "Completed 21.\n",
            "Completed 22.\n",
            "Completed 23.\n",
            "Completed 24.\n",
            "Completed 25.\n",
            "Completed 26.\n",
            "Completed 27.\n",
            "Completed 28.\n",
            "Completed 29.\n",
            "Completed 30.\n",
            "Completed 31.\n",
            "Completed 32.\n",
            "Completed 33.\n",
            "Completed 34.\n",
            "Completed 35.\n",
            "Completed 36.\n",
            "Completed 37.\n",
            "Completed 38.\n",
            "Completed 39.\n",
            "Completed 40.\n",
            "Completed 41.\n",
            "Completed 42.\n",
            "Completed 43.\n",
            "Completed 44.\n",
            "Completed 45.\n",
            "Completed 46.\n",
            "Completed 47.\n",
            "Completed 48.\n",
            "Completed 49.\n",
            "Completed 50.\n",
            "Completed 51.\n",
            "Completed 52.\n",
            "Completed 53.\n",
            "Completed 54.\n",
            "Completed 55.\n",
            "Completed 56.\n",
            "Completed 57.\n",
            "Completed 58.\n",
            "Completed 59.\n",
            "Completed 60.\n",
            "Completed 61.\n",
            "Completed 62.\n",
            "Completed 63.\n",
            "Completed 64.\n",
            "Completed 65.\n",
            "Completed 66.\n",
            "Completed 67.\n",
            "Completed 68.\n",
            "Completed 69.\n",
            "Completed 70.\n",
            "Completed 71.\n",
            "Completed 72.\n",
            "Completed 73.\n",
            "Completed 74.\n",
            "Completed 75.\n",
            "Completed 76.\n",
            "Completed 77.\n",
            "Completed 78.\n",
            "Completed 79.\n",
            "Completed 80.\n",
            "Completed 81.\n",
            "Completed 82.\n",
            "Completed 83.\n",
            "Completed 84.\n",
            "Completed 85.\n",
            "Completed 86.\n",
            "Completed 87.\n",
            "Completed 88.\n",
            "Completed 89.\n",
            "Completed 90.\n",
            "Completed 91.\n",
            "Completed 92.\n",
            "Completed 93.\n",
            "Completed 94.\n",
            "Completed 95.\n",
            "Completed 96.\n",
            "Completed 97.\n",
            "Completed 98.\n",
            "Completed 99.\n",
            "Completed 100.\n",
            "Completed 101.\n",
            "Completed 102.\n",
            "Completed 103.\n",
            "Completed 104.\n",
            "Completed 105.\n",
            "Completed 106.\n",
            "Completed 107.\n",
            "Completed 108.\n",
            "Completed 109.\n",
            "Completed 110.\n",
            "Completed 111.\n",
            "Completed 112.\n",
            "Completed 113.\n",
            "Completed 114.\n",
            "Completed 115.\n",
            "Completed 116.\n",
            "Completed 117.\n",
            "Completed 118.\n",
            "Completed 119.\n",
            "Completed 120.\n",
            "Completed 121.\n",
            "Completed 122.\n",
            "Completed 123.\n",
            "Completed 124.\n",
            "Completed 125.\n",
            "Completed 126.\n",
            "Completed 127.\n",
            "Completed 128.\n",
            "Completed 129.\n",
            "Completed 130.\n",
            "Completed 131.\n",
            "Completed 132.\n",
            "Completed 133.\n",
            "Completed 134.\n",
            "Completed 135.\n",
            "Completed 136.\n",
            "Completed 137.\n",
            "Completed 138.\n",
            "Completed 139.\n",
            "Completed 140.\n",
            "Completed 141.\n",
            "Completed 142.\n",
            "Completed 143.\n",
            "Completed 144.\n",
            "Completed 145.\n",
            "Completed 146.\n",
            "Completed 147.\n",
            "Completed 148.\n",
            "Completed 149.\n",
            "Completed 150.\n",
            "Completed 151.\n",
            "Completed 152.\n",
            "Completed 153.\n",
            "Completed 154.\n",
            "Completed 155.\n",
            "Completed 156.\n",
            "Completed 157.\n",
            "Completed 158.\n",
            "Completed 159.\n",
            "Completed 160.\n",
            "Completed 161.\n",
            "Completed 162.\n",
            "Completed 163.\n",
            "Completed 164.\n",
            "Completed 165.\n",
            "Completed 166.\n",
            "Completed 167.\n",
            "Completed 168.\n",
            "Completed 169.\n",
            "Completed 170.\n",
            "Completed 171.\n",
            "Completed 172.\n",
            "Completed 173.\n",
            "Completed 174.\n",
            "Completed 175.\n",
            "Completed 176.\n",
            "Completed 177.\n",
            "Completed 178.\n",
            "Completed 179.\n",
            "Completed 180.\n",
            "Completed 181.\n",
            "Completed 182.\n",
            "Completed 183.\n",
            "Completed 184.\n",
            "Completed 185.\n",
            "Completed 186.\n",
            "Completed 187.\n",
            "Completed 188.\n",
            "Completed 189.\n",
            "Completed 190.\n",
            "Completed 191.\n",
            "Completed 192.\n",
            "Completed 193.\n",
            "Completed 194.\n",
            "Completed 195.\n",
            "Completed 196.\n",
            "Completed 197.\n",
            "Completed 198.\n",
            "Completed 199.\n",
            "Completed 200.\n",
            "Completed 201.\n",
            "Completed 202.\n",
            "Completed 203.\n",
            "Completed 204.\n",
            "Completed 205.\n",
            "Completed 206.\n",
            "Completed 207.\n",
            "Completed 208.\n",
            "Completed 209.\n",
            "Completed 210.\n",
            "Completed 211.\n",
            "Completed 212.\n",
            "Completed 213.\n",
            "Completed 214.\n",
            "Completed 215.\n",
            "Completed 216.\n",
            "Completed 217.\n",
            "Completed 218.\n",
            "Completed 219.\n",
            "Completed 220.\n",
            "Completed 221.\n",
            "Completed 222.\n",
            "Completed 223.\n",
            "Completed 224.\n",
            "Completed 225.\n",
            "Completed 226.\n",
            "Completed 227.\n",
            "Completed 228.\n",
            "Completed 229.\n",
            "Completed 230.\n",
            "Completed 231.\n",
            "Completed 232.\n",
            "Completed 233.\n",
            "Completed 234.\n",
            "Completed 235.\n",
            "Completed 236.\n",
            "Completed 237.\n",
            "Completed 238.\n",
            "Completed 239.\n",
            "Completed 240.\n",
            "Completed 241.\n",
            "Completed 242.\n",
            "Completed 243.\n",
            "Completed 244.\n",
            "Completed 245.\n",
            "Completed 246.\n",
            "Completed 247.\n",
            "Completed 248.\n",
            "Completed 249.\n",
            "Completed 250.\n",
            "Completed 251.\n",
            "Completed 252.\n",
            "Completed 253.\n",
            "Completed 254.\n",
            "Completed 255.\n",
            "Completed 256.\n",
            "Completed 257.\n",
            "Completed 258.\n",
            "Completed 259.\n",
            "Completed 260.\n",
            "Completed 261.\n",
            "Completed 262.\n",
            "Completed 263.\n",
            "Completed 264.\n",
            "Completed 265.\n",
            "Completed 266.\n",
            "Completed 267.\n",
            "Completed 268.\n",
            "Completed 269.\n",
            "Completed 270.\n",
            "Completed 271.\n",
            "Completed 272.\n",
            "Completed 273.\n",
            "Completed 274.\n",
            "Completed 275.\n",
            "Completed 276.\n",
            "Completed 277.\n",
            "Completed 278.\n",
            "Completed 279.\n",
            "Completed 280.\n",
            "Completed 281.\n",
            "Completed 282.\n",
            "Completed 283.\n",
            "Completed 284.\n",
            "Completed 285.\n",
            "Completed 286.\n",
            "Completed 287.\n",
            "Completed 288.\n",
            "Completed 289.\n",
            "Completed 290.\n",
            "Completed 291.\n",
            "Completed 292.\n",
            "Completed 293.\n",
            "Completed 294.\n",
            "Completed 295.\n",
            "Completed 296.\n",
            "Completed 297.\n",
            "Completed 298.\n",
            "Completed 299.\n",
            "Completed 300.\n",
            "Completed 301.\n",
            "Completed 302.\n",
            "Completed 303.\n",
            "Completed 304.\n",
            "Completed 305.\n",
            "Completed 306.\n",
            "Completed 307.\n",
            "Completed 308.\n",
            "Completed 309.\n",
            "Completed 310.\n",
            "Completed 311.\n",
            "Completed 312.\n",
            "Completed 313.\n",
            "Completed 314.\n",
            "Completed 315.\n",
            "Completed 316.\n",
            "Completed 317.\n",
            "Completed 318.\n",
            "Completed 319.\n",
            "Completed 320.\n",
            "Completed 321.\n",
            "Completed 322.\n",
            "Completed 323.\n",
            "Completed 324.\n",
            "Completed 325.\n",
            "Completed 326.\n",
            "Completed 327.\n",
            "Completed 328.\n",
            "Completed 329.\n",
            "Completed 330.\n",
            "Completed 331.\n",
            "Completed 332.\n",
            "Completed 333.\n",
            "Completed 334.\n",
            "Completed 335.\n",
            "Completed 336.\n",
            "Completed 337.\n",
            "Completed 338.\n",
            "Completed 339.\n",
            "Completed 340.\n",
            "Completed 341.\n",
            "Completed 342.\n",
            "Completed 343.\n",
            "Completed 344.\n",
            "Completed 345.\n",
            "Completed 346.\n",
            "Completed 347.\n",
            "Completed 348.\n",
            "Completed 349.\n",
            "Completed 350.\n",
            "Completed 351.\n",
            "Completed 352.\n",
            "Completed 353.\n",
            "Completed 354.\n",
            "Completed 355.\n",
            "Completed 356.\n",
            "Completed 357.\n",
            "Completed 358.\n",
            "Completed 359.\n",
            "Completed 360.\n",
            "Completed 361.\n",
            "Completed 362.\n",
            "Completed 363.\n",
            "Completed 364.\n",
            "Completed 365.\n",
            "Completed 366.\n",
            "Completed 367.\n",
            "Completed 368.\n",
            "Completed 369.\n",
            "Completed 370.\n",
            "Completed 371.\n",
            "Completed 372.\n",
            "Completed 373.\n",
            "Completed 374.\n",
            "Completed 375.\n",
            "Completed 376.\n",
            "Completed 377.\n",
            "Completed 378.\n",
            "Completed 379.\n",
            "Completed 380.\n",
            "Completed 381.\n",
            "Completed 382.\n",
            "Completed 383.\n",
            "Completed 384.\n",
            "Completed 385.\n",
            "Completed 386.\n",
            "Completed 387.\n",
            "Completed 388.\n",
            "Completed 389.\n",
            "Completed 390.\n",
            "Completed 391.\n",
            "Completed 392.\n",
            "Completed 393.\n",
            "Completed 394.\n",
            "Completed 395.\n",
            "Completed 396.\n",
            "Completed 397.\n",
            "Completed 398.\n",
            "Completed 399.\n",
            "Completed 400.\n",
            "Completed 401.\n",
            "Completed 402.\n",
            "Completed 403.\n",
            "Completed 404.\n",
            "Completed 405.\n",
            "Completed 406.\n",
            "Completed 407.\n",
            "Completed 408.\n",
            "Completed 409.\n",
            "Completed 410.\n",
            "Completed 411.\n",
            "Completed 412.\n",
            "Completed 413.\n",
            "Completed 414.\n",
            "Completed 415.\n",
            "Completed 416.\n",
            "Completed 417.\n",
            "Completed 418.\n",
            "Completed 419.\n",
            "Completed 420.\n",
            "Completed 421.\n",
            "Completed 422.\n",
            "Completed 423.\n",
            "Completed 424.\n",
            "Completed 425.\n",
            "Completed 426.\n",
            "Completed 427.\n",
            "Completed 428.\n",
            "Completed 429.\n",
            "Completed 430.\n",
            "Completed 431.\n",
            "Completed 432.\n",
            "Completed 433.\n",
            "Completed 434.\n",
            "Completed 435.\n",
            "Completed 436.\n",
            "Completed 437.\n",
            "Completed 438.\n",
            "Completed 439.\n",
            "Completed 440.\n",
            "Completed 441.\n",
            "Completed 442.\n",
            "Completed 443.\n",
            "Completed 444.\n",
            "Completed 445.\n",
            "Completed 446.\n",
            "Completed 447.\n",
            "Completed 448.\n",
            "Completed 449.\n",
            "Completed 450.\n",
            "Completed 451.\n",
            "Completed 452.\n",
            "Completed 453.\n",
            "Completed 454.\n",
            "Completed 455.\n",
            "Completed 456.\n",
            "Completed 457.\n",
            "Completed 458.\n",
            "Completed 459.\n",
            "Completed 460.\n",
            "Completed 461.\n",
            "Completed 462.\n",
            "Completed 463.\n",
            "Completed 464.\n",
            "Completed 465.\n",
            "Completed 466.\n",
            "Completed 467.\n",
            "Completed 468.\n",
            "Completed 469.\n",
            "Completed 470.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to a file\n",
        "with open(\"fine_tuned_evaluation.json\", \"w\") as outfile:\n",
        "    json.dump(results, outfile, indent=4)\n",
        "\n",
        "# Output average ROUGE-L metric\n",
        "average_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
        "print(f\"Average ROUGE-L Metric: {average_rouge_l}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_khg7Ysmeybm",
        "outputId": "8b46dd4f-8d2d-47bf-c197-3198e0985328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average ROUGE-L Metric: 0.3035444978283638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras & KerasNLP\n",
        "# Install Keras 3 last, see https://keras.io/getting_started\n",
        "%pip install --upgrade --quiet keras-nlp\n",
        "%pip install --upgrade --quiet keras\n",
        "\n",
        "# Hugging Face Transformers\n",
        "%pip install --upgrade --quiet accelerate sentencepiece transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuPZznza52k5",
        "outputId": "eabce3bd-0c90-4894-d630-468c9bbf5442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetuned model\n",
        "MODEL_NAME = \"MODEL_1\"\n",
        "FINETUNED_MODEL_DIR = MODEL_NAME\n",
        "FINETUNED_WEIGHTS_PATH = f\"{FINETUNED_MODEL_DIR}/model.weights.h5\"\n",
        "FINETUNED_VOCAB_PATH = f\"{FINETUNED_MODEL_DIR}/vocabulary.spm\"\n",
        "\n",
        "# Converted model\n",
        "HUGGINGFACE_MODEL_DIR = f\"{MODEL_NAME}_huggingface\"\n"
      ],
      "metadata": {
        "id": "u1h14m256Si7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from numba import cuda\n",
        "\n",
        "# Make sure the directory exists\n",
        "%mkdir -p $FINETUNED_MODEL_DIR\n",
        "\n",
        "gemma_lm.save_weights(FINETUNED_WEIGHTS_PATH)\n",
        "\n",
        "gemma_lm.preprocessor.tokenizer.save_assets(FINETUNED_MODEL_DIR)\n",
        "\n"
      ],
      "metadata": {
        "id": "aKb-HJck6sZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del gemma_lm\n",
        "\n",
        "device = cuda.get_current_device()\n",
        "cuda.select_device(device.id)\n",
        "cuda.close()"
      ],
      "metadata": {
        "id": "5U5oVzWO69bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the conversion script from KerasNLP tools\n",
        "!wget -nv -nc https://raw.githubusercontent.com/keras-team/keras-nlp/master/tools/gemma/export_gemma_to_hf.py\n",
        "\n",
        "# Run the conversion script\n",
        "# Note: it uses the PyTorch backend of Keras (hence the KERAS_BACKEND env variable)\n",
        "!KERAS_BACKEND=torch python export_gemma_to_hf.py \\\n",
        "    --weights_file $FINETUNED_WEIGHTS_PATH \\\n",
        "    --size $MODEL_SIZE \\\n",
        "    --vocab_path $FINETUNED_VOCAB_PATH \\\n",
        "    --output_dir $HUGGINGFACE_MODEL_DIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyt1GVCI7C2p",
        "outputId": "0d1d007c-bca8-436c-ac7f-ca45af5a3461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-24 23:25:16 URL:https://raw.githubusercontent.com/keras-team/keras-nlp/master/tools/gemma/export_gemma_to_hf.py [11761/11761] -> \"export_gemma_to_hf.py\" [1]\n",
            "2024-05-24 23:25:20.564667: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "FATAL Flags parsing error: flag --size=None: Flag --size must have a value other than None.\n",
            "Pass --helpshort or --helpfull to see help on flags.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "BdrtFwQy7V6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inference with Transformers\n",
        "#Before deploying the converted model, test it using the transformers library.\n",
        "\n",
        "#Load the model and the tokenizer:\n",
        "\n",
        "model = transformers.GemmaForCausalLM.from_pretrained(\n",
        "    HUGGINGFACE_MODEL_DIR,\n",
        "    local_files_only=True,\n",
        "    device_map=\"auto\",  # Library \"accelerate\" to auto-select GPU\n",
        ")\n",
        "tokenizer = transformers.GemmaTokenizer.from_pretrained(\n",
        "    HUGGINGFACE_MODEL_DIR,\n",
        "    local_files_only=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "5DW0jyzd7JKk",
        "outputId": "43fa9232-597f-48c4-82cd-4b0d575f1050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like MODEL_1_huggingface is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1222\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1816\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m         raise LocalEntryNotFoundError(\n\u001b[0m\u001b[1;32m   1818\u001b[0m             \u001b[0;34m\"Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-5fc20d6af67f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Load the model and the tokenizer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model = transformers.GemmaForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mHUGGINGFACE_MODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3157\u001b[0m             \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3158\u001b[0;31m             config, model_kwargs = cls.config_class.from_pretrained(\n\u001b[0m\u001b[1;32m   3159\u001b[0m                 \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_token_in_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             logger.warning(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    690\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m         ):\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0;34mf\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this file, couldn't find it in the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;34mf\" cached files and it looks like {path_or_repo_id} is not the path to a directory containing a file named\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like MODEL_1_huggingface is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "def test_transformers_model(\n",
        "    model: transformers.GemmaForCausalLM,\n",
        "    tokenizer: transformers.GemmaTokenizer,\n",
        ") -> None:\n",
        "    for prompt in TEST_PROMPTS:\n",
        "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
        "        outputs = model.generate(**inputs, max_length=30)\n",
        "\n",
        "        output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"{output}\\n{'- '*40}\")\n",
        "\n",
        "\n",
        "test_transformers_model(model, tokenizer)\n"
      ],
      "metadata": {
        "id": "i_8rvee_5yOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Release resources\n",
        "del model, tokenizer\n",
        "\n",
        "# Free GPU RAM\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Restore the default encoding (current issue with the transformers library)\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "zVyTY3pq8FeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yd-1cNw1dTn"
      },
      "source": [
        "## Inference after fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probability Prompt"
      ],
      "metadata": {
        "id": "HGkYm_ldxWdL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7cDJHy8WfCB",
        "outputId": "dc651dde-f1c4-4237-939c-8d8cbd00932d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "What is the difference between permutations and combinations?\n",
            "\n",
            "Response:\n",
            "In probability and statistics, a permutation is an ordered list of distinct values chosen from a fixed set, while a combination is a non-ordered subset of a given set. Permutations are often used to model the order of outcomes in a random experiment, while combinations are used to count the distinct subsets of a set. The difference between permutations and combinations can be illustrated using the example of choosing 5 cards from a standard deck of 52 cards without replacement:\n",
            "\n",
            "Permutation: The order of the cards does not matter, so there are 5! = 120 possible permutations.\n",
            "\n",
            "Combination: The order of the cards matters, so there are only 5C5 = 5 different combinations.\n"
          ]
        }
      ],
      "source": [
        "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "prompt = template.format(\n",
        "    instruction=\"What is the difference between permutations and combinations?\",\n",
        "    response=\"\",\n",
        ")\n",
        "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7nVd8Mi1Yta"
      },
      "source": [
        "### Supervised Learning Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-2sYl2jqwl7",
        "outputId": "9d030542-2f77-4ba3-cbb1-84b3fa292763",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "What is Supervised Learning?\n",
            "\n",
            "Response:\n",
            "Supervised Learning is the process of learning from a labeled training set. It involves training an algorithm on a set of input-output pairs to predict the output from the input, based on the knowledge obtained from the training data. The algorithm is trained using the labeled examples from the training data and is then tested on an independent test set to see how well it generalizes to new data.\n"
          ]
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"What is Supervised Learning?\",\n",
        "    response=\"\",\n",
        ")\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8kFG12l0mVe"
      },
      "source": [
        "To get better responses from the fine-tuned model, you can experiment with:\n",
        "\n",
        "1. Increasing the size of the fine-tuning dataset\n",
        "2. Training for more steps (epochs)\n",
        "3. Setting a higher LoRA rank\n",
        "4. Modifying the hyperparameter values such as `learning_rate` and `weight_decay`.\n",
        "\n",
        "Try Alpaca's configuration below\n",
        "\n",
        "| Hyperparameter | LLaMA-7B | LLaMA-13B |\n",
        "|----------------|----------|-----------|\n",
        "| Batch size     | 128      | 128       |\n",
        "| Learning rate  | 2e-5     | 1e-5      |\n",
        "| Epochs         | 3        | 5         |\n",
        "| Max length     | 512      | 512       |\n",
        "| Weight decay   | 0        | 0         |\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_TO_GITHUB:\n",
        "    !git add {FILE_NAME}\n",
        "    !git config --global user.email {GIT_USER_EMAIL}\n",
        "    !git config --global user.name {GIT_USER_NAME}\n",
        "    !git commit -am \"update {FILE_NAME}\"\n",
        "    !git push"
      ],
      "metadata": {
        "id": "CHlzum8jt_P1",
        "outputId": "b0a1d755-bf59-4b3c-d23d-518a3d404149",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31m=3\u001b[m\n",
            "\t\u001b[31m__pycache__/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n",
            "remote: Permission to alinemsm/CS221-project.git denied to alinemsm.\n",
            "fatal: unable to access 'https://github.com/alinemsm/CS221-project.git/': The requested URL returned error: 403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_TO_GITHUB:\n",
        "    !git add \"{FILE_NAME}\"\n",
        "    !git config --global user.email \"{GIT_USER_EMAIL}\"\n",
        "    !git config --global user.name \"{GIT_USER_NAME}\"\n",
        "    !git commit -am \"update {FILE_NAME}\"\n",
        "\n",
        "\n",
        "    # Authenticate using GitHub token\n",
        "    !git remote set-url origin \"https://{GIT_USER_NAME}:{GIT_TOKEN}@github.com/{GIT_USER_NAME}/{GIT_REPOSITORY}.git\"\n",
        "\n",
        "    !git push"
      ],
      "metadata": {
        "id": "qotg8tjb00Sa",
        "outputId": "0dfcaf74-4785-4a03-f676-2fb55da7600b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31m=3\u001b[m\n",
            "\t\u001b[31m__pycache__/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n",
            "remote: Permission to alinemsm/CS221-project.git denied to alinemsm.\n",
            "fatal: unable to access 'https://github.com/alinemsm/CS221-project.git/': The requested URL returned error: 403\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}