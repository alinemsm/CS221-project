{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDEExiAk4fLb"
   },
   "source": [
    "# Fine-tune Gemma 2b using LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1q6-W_mKIT-"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0_EdOg9DPK6Q"
   },
   "source": [
    "import os\n",
    "from google.colab import userdata, drive"
   ],
   "outputs": []
  },
  {
   "metadata": {
    "id": "eAOqB4S6vv8b"
   },
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "COLAB = True\n",
    "KAGGLE = True\n",
    "DOWNLOAD_DATA = True\n",
    "SAVE_TO_GITHUB = True\n",
    "GIT_REPOSITORY = \"CS221-project\"\n",
    "FILE_NAME = \"colab_tuning_legacy.ipynb\"\n"
   ],
   "outputs": []
  },
  {
   "metadata": {
    "id": "fKsL8ZfCvv8b",
    "outputId": "490cef7c-1cb6-4f27-bc2f-fdb569425a50",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "if COLAB:\n",
    "    %cd /content\n",
    "    drive.mount('/content/drive', force_remount=True)"
   ],
   "outputs": []
  },
  {
   "metadata": {
    "id": "IKHYlV8ovv8b",
    "outputId": "ac597ee2-5eac-4a95-f660-ea522d532707",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "if COLAB:\n",
    "    PARENT_DIRECTORY_PATH = \"/content\"\n",
    "    # In case you want to clone in your drive:\n",
    "    PARENT_DIRECTORY_PATH = \"/content/drive/MyDrive\"\n",
    "    PROJECT_PATH = PARENT_DIRECTORY_PATH + \"/\" + GIT_REPOSITORY\n",
    "    %cd \"{PARENT_DIRECTORY_PATH}\""
   ],
   "outputs": []
  },
  {
   "metadata": {
    "id": "ycjSLQITvv8c",
    "outputId": "e0b216f6-7cd7-4dc5-9d46-e0e2271ffa35",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "if COLAB:\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    with open(f\"{PARENT_DIRECTORY_PATH}/Git/git.json\", \"r\") as f:\n",
    "        parsed_json = json.load(f)\n",
    "\n",
    "    GIT_USER_NAME = parsed_json[\"GIT_USER_NAME\"]\n",
    "    GIT_TOKEN = parsed_json[\"GIT_TOKEN\"]\n",
    "    GIT_USER_EMAIL = parsed_json[\"GIT_USER_EMAIL\"]\n",
    "\n",
    "    GIT_PATH = (\n",
    "        f\"https://{GIT_TOKEN}@github.com/{GIT_USER_NAME}/{GIT_REPOSITORY}.git\"\n",
    "    )\n",
    "\n",
    "    %cd \"{PARENT_DIRECTORY_PATH}\"\n",
    "\n",
    "    if os.path.exists(f\"{PARENT_DIRECTORY_PATH}/{GIT_REPOSITORY}\"):\n",
    "        %cd \"{PROJECT_PATH}\"\n",
    "        !git pull\n",
    "    else:\n",
    "        !git clone \"{GIT_PATH}\"  # Clone the github repository\n",
    "        %cd \"{PROJECT_PATH}\""
   ],
   "outputs": []
  },
  {
   "metadata": {
    "id": "ntzzF8ouvv8c"
   },
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "if COLAB:\n",
    "    import os\n",
    "    os.environ[\"KAGGLE_CONFIG_DIR\"] = f\"{PARENT_DIRECTORY_PATH}/Kaggle/kaggle.json\""
   ],
   "outputs": []
  },
  {
   "metadata": {
    "id": "8qNLYh3Ivv8d"
   },
   "cell_type": "markdown",
   "source": [
    "### Set environment variables"
   ]
  },
  {
   "metadata": {
    "id": "WnmItm6gvv8d"
   },
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
    "# os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
    "\n",
    "# Read the kaggle.json file\n",
    "# with open(\"kaggle.json\") as f:\n",
    "#     kaggle_info = json.load(f)\n",
    "\n",
    "# Set the environment variables\n",
    "# os.environ[\"KAGGLE_USERNAME\"] = kaggle_info[\"username\"]\n",
    "# os.environ[\"KAGGLE_KEY\"] = kaggle_info[\"key\"]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuEUAKJW1QkQ"
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1eeBtYqJsZPG",
    "outputId": "fbbd924d-c45a-4114-d666-d7a57bfd1793",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGLS-l5TxIR4"
   },
   "source": [
    "### Select a backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yn5uy8X8sdD0"
   },
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
    "# Avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZs8XXqUKRmi"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FYHyPUA9hKTf"
   },
   "source": [
    "import keras\n",
    "import keras_nlp"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T7xe_jzslv4"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45UpBDfBgf0I"
   },
   "source": [
    "Preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install python-dotenv"
   ],
   "metadata": {
    "id": "LHBvCsU3zPWK",
    "outputId": "0ce9f58f-85aa-47c5-972d-8f4c138577be",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install anthropic"
   ],
   "metadata": {
    "id": "8AELgZC9zD09",
    "outputId": "cd2a4a7f-720b-49d5-b436-cf144d1c1b16",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZiS-KU9osh_N",
    "outputId": "fd501818-73e7-47c0-c3e3-d7a441be3420",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "ExecuteTime": {
     "end_time": "2024-05-24T16:27:16.858163Z",
     "start_time": "2024-05-24T16:27:16.649042Z"
    }
   },
   "source": [
    "from utils import preprocess_qa_data\n",
    "with open(\"qa_data.txt\") as file:\n",
    "        content = file.read()"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T16:27:18.543867Z",
     "start_time": "2024-05-24T16:27:18.534382Z"
    }
   },
   "cell_type": "code",
   "source": "data = preprocess_qa_data(content)",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RCE3fdGhDE5"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vz5zLEyLstfn",
    "outputId": "d8d07c82-c87a-44c4-921a-101420596343",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660
    }
   },
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
    "gemma_lm.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_L6A5J-1QgC"
   },
   "source": [
    "## Inference before fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVLXadptyo34"
   },
   "source": [
    "### Probability Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwQz3xxxKciD",
    "outputId": "871d4e0a-e707-4362-cea9-aca3a6715384",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "\n",
    "prompt = template.format(\n",
    "    instruction=\"What is the difference between permutations and combinations?\",\n",
    "    response=\"\",\n",
    ")\n",
    "\n",
    "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
    "gemma_lm.compile(sampler=sampler)\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQ74Zz_S0iVv"
   },
   "source": [
    "### Supervised Learning Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lorJMbsusgoo",
    "outputId": "7acc803e-3d88-4174-e6e4-48457b1684c5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"What is Supervised Learning?\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt7Nr6a7tItO"
   },
   "source": [
    "## LoRA Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCucu6oHz53G",
    "outputId": "5858049f-ce60-4c15-cc4a-07416a493585",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    }
   },
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_lm.backbone.enable_lora(rank=4)\n",
    "gemma_lm.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Peq7TnLtHse",
    "outputId": "8be74883-328e-46e7-d1dc-6521b786a45c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Limit the input sequence length to 512 (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = 512\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "gemma_lm.fit(data, epochs=1, batch_size=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the fine-tuned model\n",
    "gemma_lm.save(\"/content/drive/MyDrive/Colab Notebooks/cs221/fine_tuned_model_1.keras\")\n"
   ],
   "metadata": {
    "id": "3f_W8l02mukQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0lHxEDX03gp"
   },
   "source": [
    "# Uncomment the line below if you want to enable mixed precision training on GPUs\n",
    "# keras.mixed_precision.set_global_policy('mixed_bfloat16')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the fine-tuned model\n",
    "\n",
    "# loaded_model = keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/cs221/fine_tuned_model_1.keras\")\n",
    "\n",
    "# Use the loaded model for generation\n",
    "# prompt = template.format(\n",
    "#     instruction=\"What is Supervised Learning?\",\n",
    "#     response=\"\",\n",
    "# )\n",
    "# sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
    "# loaded_model.compile(sampler=sampler)\n",
    "# generated_text = loaded_model.generate(prompt, max_length=256)\n",
    "# print(generated_text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yTLaxefSnBFb",
    "outputId": "56e065e5-5ff5-4429-d4e1-291994099a6a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yd-1cNw1dTn"
   },
   "source": [
    "## Inference after fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Probability Prompt"
   ],
   "metadata": {
    "id": "HGkYm_ldxWdL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7cDJHy8WfCB",
    "outputId": "bad71b96-e357-4156-cefe-67cd4f26dff1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "prompt = template.format(\n",
    "    instruction=\"What is the difference between permutations and combinations?\",\n",
    "    response=\"\",\n",
    ")\n",
    "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
    "gemma_lm.compile(sampler=sampler)\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7nVd8Mi1Yta"
   },
   "source": [
    "### Supervised Learning Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-2sYl2jqwl7",
    "outputId": "389024a6-c6c0-4417-b925-7133edd762f7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"What is Supervised Learning?\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8kFG12l0mVe"
   },
   "source": [
    "To get better responses from the fine-tuned model, you can experiment with:\n",
    "\n",
    "1. Increasing the size of the fine-tuning dataset\n",
    "2. Training for more steps (epochs)\n",
    "3. Setting a higher LoRA rank\n",
    "4. Modifying the hyperparameter values such as `learning_rate` and `weight_decay`.\n",
    "\n",
    "Try Alpaca's configuration below\n",
    "\n",
    "| Hyperparameter | LLaMA-7B | LLaMA-13B |\n",
    "|----------------|----------|-----------|\n",
    "| Batch size     | 128      | 128       |\n",
    "| Learning rate  | 2e-5     | 1e-5      |\n",
    "| Epochs         | 3        | 5         |\n",
    "| Max length     | 512      | 512       |\n",
    "| Weight decay   | 0        | 0         |\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "if SAVE_TO_GITHUB:\n",
    "    !git add {FILE_NAME}\n",
    "    !git config --global user.email {GIT_USER_EMAIL}\n",
    "    !git config --global user.name {GIT_USER_NAME}\n",
    "    !git commit -am \"update {FILE_NAME}\"\n",
    "    !git push"
   ],
   "metadata": {
    "id": "CHlzum8jt_P1",
    "outputId": "b0a1d755-bf59-4b3c-d23d-518a3d404149",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if SAVE_TO_GITHUB:\n",
    "    !git add \"{FILE_NAME}\"\n",
    "    !git config --global user.email \"{GIT_USER_EMAIL}\"\n",
    "    !git config --global user.name \"{GIT_USER_NAME}\"\n",
    "    !git commit -am \"update {FILE_NAME}\"\n",
    "\n",
    "\n",
    "    # Authenticate using GitHub token\n",
    "    !git remote set-url origin \"https://{GIT_USER_NAME}:{GIT_TOKEN}@github.com/{GIT_USER_NAME}/{GIT_REPOSITORY}.git\"\n",
    "\n",
    "    !git push"
   ],
   "metadata": {
    "id": "qotg8tjb00Sa",
    "outputId": "0dfcaf74-4785-4a03-f676-2fb55da7600b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 22,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
