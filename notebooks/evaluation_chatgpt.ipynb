{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T10:09:25.339146Z",
     "start_time": "2024-06-02T10:09:23.588552Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install openai==0.28",
   "id": "ad7f76bb69355d32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28\r\n",
      "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: requests>=2.20 in /Users/al/miniconda3/envs/cs221-project/lib/python3.8/site-packages (from openai==0.28) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm in /Users/al/miniconda3/envs/cs221-project/lib/python3.8/site-packages (from openai==0.28) (4.66.4)\r\n",
      "Requirement already satisfied: aiohttp in /Users/al/miniconda3/envs/cs221-project/lib/python3.8/site-packages (from openai==0.28) (3.9.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/al/miniconda3/envs/cs221-project/lib/python3.8/site-packages (from requests>=2.20->openai==0.28) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/al/miniconda3/envs/cs221-project/lib/python3.8/site-packages (from requests>=2.20->openai==0.28) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/al/miniconda3/envs/cs221-project/lib/python3.8/site-packages (from requests>=2.20->openai==0.28) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/al/miniconda3/envs/cs221-project/lib/python3.8/site-packages (from requests>=2.20->openai==0.28) (2024.2.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/al/miniconda3/envs/cs221-project/lib/python3.8/site-packages (from aiohttp->openai==0.28) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/al/miniconda3/envs/cs221-project/lib/python3.8/site-packages (from aiohttp->openai==0.28) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/al/miniconda3/envs/cs221-project/lib/python3.8/site-packages (from aiohttp->openai==0.28) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/al/miniconda3/envs/cs221-project/lib/python3.8/site-packages (from aiohttp->openai==0.28) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/al/miniconda3/envs/cs221-project/lib/python3.8/site-packages (from aiohttp->openai==0.28) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/al/miniconda3/envs/cs221-project/lib/python3.8/site-packages (from aiohttp->openai==0.28) (4.0.3)\r\n",
      "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m76.5/76.5 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: openai\r\n",
      "  Attempting uninstall: openai\r\n",
      "    Found existing installation: openai 1.30.5\r\n",
      "    Uninstalling openai-1.30.5:\r\n",
      "      Successfully uninstalled openai-1.30.5\r\n",
      "Successfully installed openai-0.28.0\r\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T22:46:39.214651Z",
     "start_time": "2024-06-02T22:46:39.006062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "from src.utils import extract_gpt_scores, load_json"
   ],
   "id": "bf319c801e3db0b6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T22:46:42.244076Z",
     "start_time": "2024-06-02T22:46:42.240487Z"
    }
   },
   "cell_type": "code",
   "source": "openai.api_key = os.getenv(\"OPENAI_API_KEY\")",
   "id": "feccbaffaa508396",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load the evaluation data",
   "id": "bf6e68c64f0d028f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T23:15:36.547557Z",
     "start_time": "2024-06-02T23:15:36.528678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline = \"data/evaluation_baseline\"\n",
    "ft = \"data/evaluation_ft\"\n",
    "ft2x = \"data/evaluation_ft2x\"\n",
    "\n",
    "data_baseline = load_json(baseline)\n",
    "data_ft = load_json(ft)\n",
    "data_ft2x = load_json(ft2x)\n",
    "    "
   ],
   "id": "d033ce019f1d8ad0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluate the answers using GPT-4",
   "id": "91b0bf83485e6588"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 16,
   "source": [
    "# Open a file to store the outputs\n",
    "output_file = open(\"outputs/evaluation_chatgpt.txt\", \"w\")\n"
   ],
   "id": "daf213e376b98cdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loop through each question and answer pair\n",
    "for i in range(len(data_baseline)):\n",
    "    question = data_ft[i][\"instruction\"]\n",
    "    answer0 = data_baseline[i][\"model_response\"]\n",
    "    answer1 = data_ft[i][\"model_response\"]\n",
    "    answer2 = data_ft2x[i][\"model_response\"]\n",
    "\n",
    "    # Prepare the prompt for ChatGPT\n",
    "    prompt = f\"\"\"Please evaluate the following three answers to the same question using the rubric below. For each answer, provide a score from 1 to 5 (5 being the highest) based on how well it meets the criteria. Indicate which answer (0 or 1 or 2) you believe is superior.\n",
    "\n",
    "Rubric:\n",
    "1. Comprehensiveness and Depth\n",
    "    - 0: Incomplete explanation, many key aspects missing.\n",
    "    - 0.5: Partial explanation, covers some key aspects but lacks depth.\n",
    "    - 1: Complete and detailed explanation, covers all key aspects thoroughly.\n",
    "2. Accuracy and Terminology\n",
    "    - 0: Incorrect use of terminology, several inaccuracies.\n",
    "    - 0.5: Basic use of terminology, some inaccuracies.\n",
    "    - 1: Precise use of terminology, accurate definitions and explanations.\n",
    "3. Clarity and Engagement\n",
    "    - 0: Unclear explanation, difficult to follow.\n",
    "    - 0.5: Somewhat clear, with parts that are hard to follow.\n",
    "    - 1: Clear and engaging, easy to follow.\n",
    "4. Self-Containment\n",
    "    - 0: Lacks necessary context, difficult to understand.\n",
    "    - 0.5: Provides some context, but may need additional information.\n",
    "    - 1: Fully self-contained, no additional context needed.\n",
    "5. Logical Structure and Flow\n",
    "    - 0: Poorly organized, with a confusing structure and flow.\n",
    "    - 0.5: Some organization, but with occasional lapses in structure and flow.\n",
    "    - 1: Well-organized, with a logical structure and smooth flow.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer 0:\n",
    "{answer0}\n",
    "\n",
    "Answer 1:\n",
    "{answer1}\n",
    "\n",
    "Answer 2:\n",
    "{answer2}\n",
    "\n",
    "Please provide your evaluation in the following format:\n",
    "\n",
    "A0: [0-5]\n",
    "A1: [0-5]\n",
    "A2: [0-5]\n",
    "Superior: [0 or 1 or 2] (0 if all are equal)\n",
    "\"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # Extract the evaluation from the response\n",
    "    evaluation = response.choices[0].message['content'].strip()\n",
    "\n",
    "    # Write the evaluation to the output file\n",
    "    output_file.write(f\"Question {i+1}:\\n{evaluation}\\n\\n\")\n",
    "\n",
    "    # Extract the superior answer and update the wins dictionary\n",
    "    # superior = int(re.search(r\"Superior: (\\d)\", evaluation).group(1))\n",
    "    # wins[superior] += 1\n",
    "    \n",
    "    print(f\"Question {i+1} completed.\")\n",
    "# Close the output file\n",
    "output_file.close()\n"
   ],
   "id": "23ccc559b6f5a66b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T08:50:28.806362Z",
     "start_time": "2024-06-03T08:50:28.797276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a dictionary of question number and question and answer\n",
    "questions = []\n",
    "for i in range(len(data_baseline)):\n",
    "    question = data_ft[i][\"instruction\"]\n",
    "    answer0 = data_baseline[i][\"model_response\"]\n",
    "    answer1 = data_ft[i][\"model_response\"]\n",
    "    answer2 = data_ft2x[i][\"model_response\"]\n",
    "\n",
    "questions.append({\"Question Number\": f\"Question {i+1}\", \"Question\": question, \"Answer 0\": answer0, \"Answer 1\": answer1, \"Answer 2\": answer2})"
   ],
   "id": "85a298c7db27016d",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T08:51:19.294273Z",
     "start_time": "2024-06-03T08:51:19.285664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save to a json file\n",
    "with open(\"outputs/evaluation_chatgpt_questions_dict.json\", \"w\") as f:\n",
    "    json.dump(questions, f)"
   ],
   "id": "1bd9144c34d758a4",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Extract the evaluation scores",
   "id": "f1befd7c4b6a17c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read the file content\n",
    "with open('outputs/evaluation_chatgpt_reviewed.txt', 'r') as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "# Extract the data into a DataFrame\n",
    "df = extract_gpt_scores(file_content)"
   ],
   "id": "6b63607aa7616c57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T01:41:22.794557Z",
     "start_time": "2024-06-03T01:41:22.789642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate number or wins for each answer\n",
    "wins = df['Superior'].value_counts().to_dict()"
   ],
   "id": "a657175aa75cac86",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T01:41:23.947571Z",
     "start_time": "2024-06-03T01:41:23.942246Z"
    }
   },
   "cell_type": "code",
   "source": "wins",
   "id": "7e35c93f38b87785",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 182, 2: 152, 0: 62, 9: 43}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T01:41:33.112280Z",
     "start_time": "2024-06-03T01:41:33.105312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# compute percentage of wins for each answer\n",
    "total = sum(wins.values())\n",
    "win_percentages = {answer: wins[answer] / total * 100 for answer in wins}\n",
    "win_percentages"
   ],
   "id": "1fd9bf369fefa885",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 41.45785876993166,\n",
       " 2: 34.62414578587699,\n",
       " 0: 14.123006833712983,\n",
       " 9: 9.79498861047836}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
