{
  "cells": [
    {
      "metadata": {
        "id": "471c3a5c8d15035",
        "outputId": "85eb8994-75cc-42f1-b419-4e6ced8f1af0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/ content'\n",
            "/content\n",
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/CS221-project\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 4 (delta 1), reused 4 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (4/4), 473.27 KiB | 4.23 MiB/s, done.\n",
            "From https://github.com/alinemsm/CS221-project\n",
            "   ed7713e..ac72141  master     -> origin/master\n",
            "Updating ed7713e..ac72141\n",
            "Fast-forward\n",
            " qa_test_data.json  | 1882 \u001b[32m++++++++++++++++++\u001b[m\n",
            " qa_train_data.json | 7510 \u001b[32m++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " 2 files changed, 9392 insertions(+)\n",
            " create mode 100644 qa_test_data.json\n",
            " create mode 100644 qa_train_data.json\n"
          ]
        }
      ],
      "execution_count": 1,
      "source": [
        "import os\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "COLAB = True\n",
        "KAGGLE = True\n",
        "DOWNLOAD_DATA = True\n",
        "SAVE_TO_GITHUB = True\n",
        "GIT_REPOSITORY = \"CS221-project\"\n",
        "FILE_NAME = \"colab_evaluation.ipynb\"\n",
        "\n",
        "if COLAB:\n",
        "    %cd / content\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "if COLAB:\n",
        "    PARENT_DIRECTORY_PATH = \"/content\"\n",
        "    # In case you want to clone in your drive:\n",
        "    PARENT_DIRECTORY_PATH = \"/content/drive/MyDrive\"\n",
        "    PROJECT_PATH = PARENT_DIRECTORY_PATH + \"/\" + GIT_REPOSITORY\n",
        "    %cd \"{PARENT_DIRECTORY_PATH}\"\n",
        "if COLAB:\n",
        "    import json\n",
        "    import os\n",
        "\n",
        "    with open(f\"{PARENT_DIRECTORY_PATH}/Git/git.json\", \"r\") as f:\n",
        "        parsed_json = json.load(f)\n",
        "\n",
        "    GIT_USER_NAME = parsed_json[\"GIT_USER_NAME\"]\n",
        "    GIT_TOKEN = parsed_json[\"GIT_TOKEN\"]\n",
        "    GIT_USER_EMAIL = parsed_json[\"GIT_USER_EMAIL\"]\n",
        "\n",
        "    GIT_PATH = (\n",
        "        f\"https://{GIT_TOKEN}@github.com/{GIT_USER_NAME}/{GIT_REPOSITORY}.git\"\n",
        "    )\n",
        "\n",
        "    %cd \"{PARENT_DIRECTORY_PATH}\"\n",
        "\n",
        "    if os.path.exists(f\"{PARENT_DIRECTORY_PATH}/{GIT_REPOSITORY}\"):\n",
        "        %cd \"{PROJECT_PATH}\"\n",
        "        !git pull\n",
        "    else:\n",
        "        !git clone \"{GIT_PATH}\"  # Clone the github repository\n",
        "        %cd \"{PROJECT_PATH}\""
      ],
      "id": "471c3a5c8d15035"
    },
    {
      "metadata": {
        "id": "fa988c39b4c1c69e",
        "outputId": "797310bf-e0b1-4356-b15a-a801d89c1af5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/570.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/570.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.5/570.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "execution_count": 2,
      "source": [
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U keras>=3\n",
        "### Select a backend\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
        "# Avoid memory fragmentation on JAX backend.\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
      ],
      "id": "fa988c39b4c1c69e"
    },
    {
      "metadata": {
        "id": "bb799b8c4a9b5661"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 3,
      "source": [
        "import keras\n",
        "import keras_nlp"
      ],
      "id": "bb799b8c4a9b5661"
    },
    {
      "metadata": {
        "id": "15dc3beb05a864da"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 8,
      "source": [
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = f\"{PARENT_DIRECTORY_PATH}/Kaggle/kaggle.json\""
      ],
      "id": "15dc3beb05a864da"
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')"
      ],
      "metadata": {
        "id": "N5mlGOGPszUc"
      },
      "id": "N5mlGOGPszUc",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7004d17de28b26c4",
        "outputId": "676f0ee8-6906-4265-e69c-d15fffa5f11f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'task.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'preprocessor.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n"
          ]
        }
      ],
      "execution_count": 5,
      "source": [
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")"
      ],
      "id": "7004d17de28b26c4"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install anthropic"
      ],
      "metadata": {
        "id": "UlUoyw5utInX",
        "outputId": "1bdb113c-f44f-4d67-9a61-cb75b52d2b74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UlUoyw5utInX",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.26.1-py3-none-any.whl (877 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/877.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/877.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m757.8/877.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m877.6/877.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from anthropic)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jiter<1,>=0.1.0 (from anthropic)\n",
            "  Downloading jiter-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.8/327.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->anthropic)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->anthropic)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.18.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic) (0.23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.7)\n",
            "Installing collected packages: jiter, h11, httpcore, httpx, anthropic\n",
            "Successfully installed anthropic-0.26.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install python-dotenv"
      ],
      "metadata": {
        "id": "497O-RAztN_G",
        "outputId": "41e62c01-5b2d-46db-9c04-cd2cc0fcf1f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "497O-RAztN_G",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install rouge"
      ],
      "metadata": {
        "id": "i-K8eo3jtWRj",
        "outputId": "449d856a-0446-4466-8c06-af3a76c85e1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "i-K8eo3jtWRj",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3bf715bb8eff25c0"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 9,
      "source": [
        "from utils import preprocess_qa_data, compute_rouge_l\n",
        "\n",
        "# Load data from JSON file\n",
        "with open(\"qa_test_data.json\", \"r\") as file:\n",
        "    data = json.load(file)"
      ],
      "id": "3bf715bb8eff25c0"
    },
    {
      "metadata": {
        "id": "4bad975158147811",
        "outputId": "65eb373d-ffe9-4a27-aa9a-9719ccbce7f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n",
            "Completed 1/470\n"
          ]
        }
      ],
      "execution_count": 12,
      "source": [
        "# Generate responses and compute ROUGE-L metric\n",
        "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "# Define the sampler\n",
        "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "gemma_lm.compile(sampler=sampler)\n",
        "\n",
        "results = []\n",
        "rouge_l_scores = []\n",
        "\n",
        "i = 0\n",
        "for item in data:\n",
        "    prompt = template.format(instruction=item[\"instruction\"], response=\"\")\n",
        "    model_response = gemma_lm.generate(prompt, max_length=256)[0]\n",
        "\n",
        "    rouge_l_score = compute_rouge_l(item[\"response\"], model_response)\n",
        "    rouge_l_scores.append(rouge_l_score)\n",
        "\n",
        "    result = {\n",
        "        \"instruction\": item[\"instruction\"],\n",
        "        \"model_response\": model_response,\n",
        "        \"original_response\": item[\"response\"],\n",
        "        \"rouge_l_score\": rouge_l_score\n",
        "    }\n",
        "    results.append(result)\n",
        "    i += 1\n",
        "    print(f\"Completed {i}/{len(data)}\")\n",
        "\n",
        "# Save results to a file\n",
        "with open(\"baseline_evaluation.json\", \"w\") as outfile:\n",
        "    json.dump(results, outfile, indent=4)\n"
      ],
      "id": "4bad975158147811"
    },
    {
      "metadata": {
        "id": "initial_id",
        "outputId": "108c63c2-3c3b-4756-8d7e-ea097c483f47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average ROUGE-L Metric: 0.0004640982235972484\n"
          ]
        }
      ],
      "execution_count": 13,
      "source": [
        "# Output average ROUGE-L metric\n",
        "average_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
        "print(f\"Average ROUGE-L Metric: {average_rouge_l}\")"
      ],
      "id": "initial_id"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model\n",
        "loaded_model = keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/cs221/fine_tuned_model_1.keras\")\n"
      ],
      "metadata": {
        "id": "2pVQg-ORJEZz",
        "outputId": "43e12843-82e6-40c7-a390-b48f66697a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "2pVQg-ORJEZz",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 4194304256 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:         8B\n              constant allocation:         0B\n        maybe_live_out allocation:    1.95GiB\n     preallocated temp allocation:    3.91GiB\n  preallocated temp fragmentation:         0B (0.00%)\n                 total allocation:    5.86GiB\n              total fragmentation:       240B (0.00%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 1.95GiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/mul\" source_file=\"/usr/local/lib/python3.10/dist-packages/keras/src/backend/jax/random.py\" source_line=19\n\t\tXLA Label: fusion\n\t\tShape: f32[256000,2048]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 1000.00MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/usr/local/lib/python3.10/dist-packages/keras/src/backend/jax/random.py\" source_line=19\n\t\tXLA Label: fusion\n\t\tShape: u32[262144000]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 1000.00MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/usr/local/lib/python3.10/dist-packages/keras/src/backend/jax/random.py\" source_line=19\n\t\tXLA Label: fusion\n\t\tShape: u32[262144000]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 1000.00MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/usr/local/lib/python3.10/dist-packages/keras/src/backend/jax/random.py\" source_line=19\n\t\tXLA Label: fusion\n\t\tShape: u32[262144000]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 1000.00MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/usr/local/lib/python3.10/dist-packages/keras/src/backend/jax/random.py\" source_line=19\n\t\tXLA Label: fusion\n\t\tShape: u32[262144000]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 16B\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/usr/local/lib/python3.10/dist-packages/keras/src/backend/jax/random.py\" source_line=19\n\t\tXLA Label: fusion\n\t\tShape: (u32[262144000], u32[262144000])\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 16B\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/usr/local/lib/python3.10/dist-packages/keras/src/backend/jax/random.py\" source_line=19\n\t\tXLA Label: fusion\n\t\tShape: (u32[262144000], u32[262144000])\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 8B\n\t\tEntry Parameter Subshape: u32[2]\n\t\t==========================\n\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-482f82069409>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the fine-tuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/cs221/fine_tuned_model_1.keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_zip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         return saving_lib.load_model(\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    150\u001b[0m             )\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             return _load_model_from_fileobj(\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# Construct the model from the configuration file in the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mObjectSharingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             model = deserialize_keras_object(\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcustom_obj_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/task.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# vanilla `keras.Model`. We override it to get a subclass instance back.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"backbone\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"backbone\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"backbone\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"backbone\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         if \"preprocessor\" in config and isinstance(\n\u001b[1;32m    150\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preprocessor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mKeras\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m     obj = serialization_lib.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcustom_obj_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# The default `from_config()` for functional models will return a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# vanilla `keras.Model`. We override it to get a subclass instance back.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/gemma/gemma_backbone.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocabulary_size, num_layers, num_query_heads, num_key_value_heads, hidden_dim, intermediate_dim, head_dim, layer_norm_epsilon, dropout, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"padding_mask\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         )\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_id_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransformer_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_nlp/src/layers/modeling/reversible_embedding.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, inputs_shape)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtie_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/random.py\u001b[0m in \u001b[0;36mnormal\u001b[0;34m(key, shape, dtype)\u001b[0m\n\u001b[1;32m    709\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_named_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_argnums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 4194304256 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:         8B\n              constant allocation:         0B\n        maybe_live_out allocation:    1.95GiB\n     preallocated temp allocation:    3.91GiB\n  preallocated temp fragmentation:         0B (0.00%)\n                 total allocation:    5.86GiB\n              total fragmentation:       240B (0.00%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 1.95GiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/mul\" source_file=\"/usr/local/lib/python3.10/dist-packages/keras/src/backend/jax/random.py\" source_line=19\n\t\tXLA Label: fusion\n\t\tShape: f32[256000,2048]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 1000.00MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/usr/local/lib/python3.10/dist-packages/keras/src/backend/jax/random.py\" source_line=19\n\t\tXLA Label: fusion\n\t\tShape: u32[262144000]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 1000.00MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/usr/local/lib/python3.10/dist-packages/keras/src/backend/jax/random.py\" source_line=19\n\t\tXLA Label: fusion\n\t\tShape: u32[262144000]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 1000.00MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/usr/local/lib/python3.10/dist-packages/keras/src/backend/jax/random.py\" source_line=19\n\t\tXLA Label: fusion\n\t\tShape: u32[262144000]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 1000.00MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/usr/local/lib/python3.10/dist-packages/keras/src/backend/jax/random.py\" source_line=19\n\t\tXLA Label: fusion\n\t\tShape: u32[262144000]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 16B\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/usr/local/lib/python3.10/dist-packages/keras/src/backend/jax/random.py\" source_line=19\n\t\tXLA Label: fusion\n\t\tShape: (u32[262144000], u32[262144000])\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 16B\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/usr/local/lib/python3.10/dist-packages/keras/src/backend/jax/random.py\" source_line=19\n\t\tXLA Label: fusion\n\t\tShape: (u32[262144000], u32[262144000])\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 8B\n\t\tEntry Parameter Subshape: u32[2]\n\t\t==========================\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the template\n",
        "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "\n",
        "# Define the sampler\n",
        "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "loaded_model.compile(sampler=sampler)\n",
        "\n",
        "results = []\n",
        "rouge_l_scores = []\n",
        "\n",
        "i = 1\n",
        "# Generate responses and compute ROUGE-L metric\n",
        "for item in data:\n",
        "    prompt = template.format(instruction=item[\"instruction\"], response=\"\")\n",
        "    model_response = loaded_model.generate(prompt, max_length=256)[0]\n",
        "\n",
        "    rouge_l_score = compute_rouge_l(item[\"response\"], model_response)\n",
        "    rouge_l_scores.append(rouge_l_score)\n",
        "\n",
        "    result = {\n",
        "        \"instruction\": item[\"instruction\"],\n",
        "        \"model_response\": model_response,\n",
        "        \"original_response\": item[\"response\"],\n",
        "        \"rouge_l_score\": rouge_l_score\n",
        "    }\n",
        "    results.append(result)\n",
        "    i += 1\n",
        "    print(f\"Completed {i}/{len(data)}\")\n",
        "\n",
        "\n",
        "\n",
        "# Save results to a file\n",
        "with open(\"fine_tuned_evaluation.json\", \"w\") as outfile:\n",
        "    json.dump(results, outfile, indent=4)"
      ],
      "metadata": {
        "id": "rrDcy0y6JdlZ"
      },
      "id": "rrDcy0y6JdlZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output average ROUGE-L metric\n",
        "average_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
        "print(f\"Average ROUGE-L Metric: {average_rouge_l}\")"
      ],
      "metadata": {
        "id": "e-krxUrEJVkZ"
      },
      "id": "e-krxUrEJVkZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}